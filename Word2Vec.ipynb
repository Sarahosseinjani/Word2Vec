{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bGq1XjUcwcWn"
      },
      "outputs": [],
      "source": [
        "# Word2Vec Implementation\n",
        "\n",
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "PU7gPapubRrU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip-p2_nAcv6l",
        "outputId": "fb01e563-7579-4557-db42-71d62e081f72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the Word2Vec model class\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, target_word, context_word):\n",
        "        target_embed = self.in_embed(target_word)\n",
        "        context_embed = self.out_embed(context_word)\n",
        "        return target_embed, context_embed"
      ],
      "metadata": {
        "id": "ihrIIAXNwgBA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "\n",
        "def train_word2vec(corpus, window_size, embedding_dim, num_epochs, learning_rate):\n",
        "    # Preprocess the corpus and build the vocabulary\n",
        "    tokenized_corpus = word_tokenize(corpus.lower())\n",
        "    vocabulary = list(set(tokenized_corpus))\n",
        "    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
        "    idx_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
        "    vocab_size = len(vocabulary)\n",
        "    # Create the target-context word pairs\n",
        "    training_pairs = []\n",
        "    for i in range(len(tokenized_corpus)):\n",
        "        target_word = tokenized_corpus[i]\n",
        "        for j in range(i - window_size, i + window_size + 1):\n",
        "            if j != i and j >= 0 and j < len(tokenized_corpus):\n",
        "                context_word = tokenized_corpus[j]\n",
        "                training_pairs.append((word_to_idx[target_word], word_to_idx[context_word]))\n",
        "    # Initialize the Word2Vec model\n",
        "    model = Word2Vec(vocab_size, embedding_dim)\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for target_word, context_word in training_pairs:\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            target_embed, context_embed = model(torch.LongTensor([target_word]), torch.LongTensor([context_word]))\n",
        "            # Compute the loss\n",
        "            loss = criterion(target_embed, context_embed)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update the model parameters\n",
        "            optimizer.step()\n",
        "            # Accumulate the loss\n",
        "            total_loss += loss.item()\n",
        "        # Print the average loss for the epoch\n",
        "        avg_loss = total_loss / len(training_pairs)\n",
        "        print(f\"Epoch: {epoch+1}, Average Loss: {avg_loss}\")\n",
        "    # Return the trained Word2Vec model\n",
        "    return model"
      ],
      "metadata": {
        "id": "DzHH19FowgEH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the main function\n",
        "\n",
        "def main():\n",
        "    # Set hyperparameters\n",
        "    corpus = \"I love to learn deep learning. It is fascinating!\"\n",
        "    window_size = 3\n",
        "    embedding_dim = 10\n",
        "    num_epochs = 50\n",
        "    learning_rate = 0.001\n",
        "    # Load and preprocess the corpus\n",
        "    tokenized_corpus = word_tokenize(corpus.lower())\n",
        "    vocabulary = list(set(tokenized_corpus))\n",
        "    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
        "    idx_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
        "    # Train the Word2Vec model\n",
        "    model = train_word2vec(corpus, window_size, embedding_dim, num_epochs, learning_rate)\n",
        "    # Evaluate the trained model using word similarity or analogy tasks\n",
        "    word_pairs = [(\"deep\", \"learning\")]\n",
        "    for word1, word2 in word_pairs:\n",
        "        idx1 = word_to_idx[word1]\n",
        "        idx2 = word_to_idx[word2]\n",
        "        embed1 = model.in_embed(torch.LongTensor([idx1])).detach().numpy()\n",
        "        embed2 = model.in_embed(torch.LongTensor([idx2])).detach().numpy()\n",
        "        similarity = cosine_similarity(embed1, embed2)[0][0]\n",
        "        print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n",
        "    # Print the learned word embeddings\n",
        "    embeddings = []\n",
        "    for i in range(len(vocabulary)):\n",
        "        word = idx_to_word[i]\n",
        "        embed = model.in_embed(torch.LongTensor([i])).detach().numpy()\n",
        "        embeddings.append((word, embed))\n",
        "    for word, embed in embeddings:\n",
        "        print(f\"Word: {word}, Embedding: {embed}\")\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), \"word2vec_model.pth\")\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgMNdXqpi7tV",
        "outputId": "1e3e1a6a-23f8-4361-c015-3b0e374a3ae7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Average Loss: 1.9363663682231196\n",
            "Epoch: 2, Average Loss: 1.9312489242465407\n",
            "Epoch: 3, Average Loss: 1.9261469609207578\n",
            "Epoch: 4, Average Loss: 1.9210604887317728\n",
            "Epoch: 5, Average Loss: 1.915989280850799\n",
            "Epoch: 6, Average Loss: 1.9109334592466\n",
            "Epoch: 7, Average Loss: 1.9058927683918565\n",
            "Epoch: 8, Average Loss: 1.9008674025535583\n",
            "Epoch: 9, Average Loss: 1.8958572138238836\n",
            "Epoch: 10, Average Loss: 1.8908620057282624\n",
            "Epoch: 11, Average Loss: 1.885881965359052\n",
            "Epoch: 12, Average Loss: 1.880916859816622\n",
            "Epoch: 13, Average Loss: 1.8759667349082452\n",
            "Epoch: 14, Average Loss: 1.8710314846701093\n",
            "Epoch: 15, Average Loss: 1.8661111091022138\n",
            "Epoch: 16, Average Loss: 1.8612055910958185\n",
            "Epoch: 17, Average Loss: 1.8563147689457293\n",
            "Epoch: 18, Average Loss: 1.851438738129757\n",
            "Epoch: 19, Average Loss: 1.8465772651963763\n",
            "Epoch: 20, Average Loss: 1.8417304820484586\n",
            "Epoch: 21, Average Loss: 1.8368982600945014\n",
            "Epoch: 22, Average Loss: 1.832080508823748\n",
            "Epoch: 23, Average Loss: 1.8272772243729345\n",
            "Epoch: 24, Average Loss: 1.82248840894964\n",
            "Epoch: 25, Average Loss: 1.8177139599014212\n",
            "Epoch: 26, Average Loss: 1.8129538650865908\n",
            "Epoch: 27, Average Loss: 1.8082079920503829\n",
            "Epoch: 28, Average Loss: 1.8034763998455472\n",
            "Epoch: 29, Average Loss: 1.7987589918904834\n",
            "Epoch: 30, Average Loss: 1.7940556892642268\n",
            "Epoch: 31, Average Loss: 1.7893664196685508\n",
            "Epoch: 32, Average Loss: 1.784691231118308\n",
            "Epoch: 33, Average Loss: 1.7800300844289638\n",
            "Epoch: 34, Average Loss: 1.7753828885378662\n",
            "Epoch: 35, Average Loss: 1.7707495286508843\n",
            "Epoch: 36, Average Loss: 1.7661300996939342\n",
            "Epoch: 37, Average Loss: 1.761524452103509\n",
            "Epoch: 38, Average Loss: 1.7569324815714802\n",
            "Epoch: 39, Average Loss: 1.7523543337980907\n",
            "Epoch: 40, Average Loss: 1.7477898277618267\n",
            "Epoch: 41, Average Loss: 1.7432389976801697\n",
            "Epoch: 42, Average Loss: 1.7387017320703577\n",
            "Epoch: 43, Average Loss: 1.7341780756358747\n",
            "Epoch: 44, Average Loss: 1.7296678512184709\n",
            "Epoch: 45, Average Loss: 1.7251711035216297\n",
            "Epoch: 46, Average Loss: 1.7206877635584936\n",
            "Epoch: 47, Average Loss: 1.7162178026305304\n",
            "Epoch: 48, Average Loss: 1.7117611396091956\n",
            "Epoch: 49, Average Loss: 1.7073177612490125\n",
            "Epoch: 50, Average Loss: 1.7028875820062779\n",
            "Similarity between 'deep' and 'learning': 0.20526504516601562\n",
            "Word: love, Embedding: [[ 1.8843113  -0.23206174  0.20253325  0.4148074   0.5950865  -1.133036\n",
            "   0.73117054 -1.2078662   0.30648243  1.9130137 ]]\n",
            "Word: i, Embedding: [[-1.7222425  -0.7111287   1.4318893  -0.06183858 -0.5107098   0.3334757\n",
            "   1.4504164  -0.4608221   0.42007974  0.15743312]]\n",
            "Word: !, Embedding: [[ 0.6549441   1.3094505   0.21903919 -0.99480826 -0.78159076 -0.29836044\n",
            "  -0.15046147 -0.04629197  1.0570877   1.1328822 ]]\n",
            "Word: deep, Embedding: [[ 0.35463828 -0.32842588  0.01219738 -0.82427835 -0.0544473   0.06409167\n",
            "   1.0188195  -0.08265517 -0.4782881   2.060179  ]]\n",
            "Word: learning, Embedding: [[ 0.55606997 -0.62646526  1.7609713  -1.3853976   0.22695951  0.6182204\n",
            "  -0.31532001  1.3248079   0.6131123   0.31428316]]\n",
            "Word: fascinating, Embedding: [[ 0.96597135 -0.7885194   0.6146244  -1.2581717  -0.54292387  0.26131704\n",
            "  -0.7915802   0.2685693   0.55298054 -0.21351507]]\n",
            "Word: ., Embedding: [[ 0.01042818  0.13683525 -0.79825234  1.132776    0.60754055 -0.36688113\n",
            "   0.3439671  -1.0932591   0.74103636  0.35113597]]\n",
            "Word: is, Embedding: [[ 2.1250074  -1.1977067  -0.9042101   0.61984223  1.7358356  -0.06060061\n",
            "  -0.79221237  0.84989244  0.942018    0.530004  ]]\n",
            "Word: to, Embedding: [[-0.62404376 -0.45646518 -0.1631578  -0.6984602  -0.51405394 -0.5248064\n",
            "   0.15285449 -0.36127985  0.07450148  1.8348771 ]]\n",
            "Word: learn, Embedding: [[-0.60161364 -0.3589982   0.38527614 -0.18932588 -1.1099533   0.5502319\n",
            "  -1.120096   -1.3926766  -0.8774818   0.9265683 ]]\n",
            "Word: it, Embedding: [[ 0.6222483  -0.9476658   0.20500578  0.74377286 -0.63399684 -0.5068059\n",
            "   0.600578   -0.21485627  0.16632973 -2.260296  ]]\n"
          ]
        }
      ]
    }
  ]
}